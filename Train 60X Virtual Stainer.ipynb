{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1B - Training of virtual staining of brightfield images (60x)\n",
    "\n",
    "Example code to train a neural network to virtually stain brightfield images captured with the 60x magnification objective obtaining the corresponding images for nuclei, lipids and cytoplasm.\n",
    "\n",
    "This code can be easily adapted to train other virtual staining neural networks by changing the neural network and training parameters in section 1.1 and \n",
    "\n",
    "version 1.0 <br />\n",
    "15 November 2020 <br />\n",
    "Benjamin Midtvedt, Jes√∫s Pineda Castro, Saga Helgadottir, Daniel Midtvedt & Giovanni Volpe <br />\n",
    "Soft Matter Lab @ GU <br />\n",
    "http://www.softmatterlab.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports\n",
    " \n",
    "Import all necessary packages. These include standard Python packages as well as the core of DeepTrack 2.0 (`deeptrack`) and some specialized classes for this virtual staining (`apido`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# DeepTrack 2.0 code\n",
    "import apido\n",
    "from apido import deeptrack as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define input and output\n",
    "\n",
    "Set constants to determine the input and output images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Neural-network model parameters\n",
    "\n",
    "Parameters of the neural network model. These are:\n",
    "\n",
    "* `GENERATOR_BREADTH`: determines the width of the input image as `GENERATOR_BREADTH * 32` (e.g., `GENERATOR_BREADTH = 32` corresponds to an input image size `532`)\n",
    "\n",
    "* `GENERATOR_DEPTH`: Depth of the generator U-Net\n",
    "\n",
    "* `DISCRIMINATOR_DEPTH`: Depth of the discriminator convolutional encoder\n",
    "\n",
    "* `MAE_LOSS_WEIGHT`: the weighting of the MAE loss vs. the adversarial loss\n",
    "\n",
    "* `EPOCHS`: number of epochs to train. We recommend the range 200-500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_BREADTH = 16\n",
    "GENERATOR_DEPTH = 5\n",
    "DISCRIMINATOR_DEPTH = 5\n",
    "MAE_LOSS_WEIGHT = 0.001\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 User-defined constants for loading data and saving model\n",
    "\n",
    "Constants defined by the user:\n",
    "\n",
    "* `DATASET_PATH`: Input path (not including the magnification folder)\n",
    "\n",
    "* `OUTPUT_PATH`: Output path (not including the magnication folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"D:/hackathon\" \n",
    "OUTPUT_PATH = \"./models/\"\n",
    "\n",
    "VALIDATION_WELLS_AND_SITES = [\n",
    "    (\"B03\", 2),\n",
    "    (\"C04\", 4),\n",
    "    (\"B04\", 3),\n",
    "    (\"C02\", 1),\n",
    "    (\"D02\", 3),\n",
    "    (\"B03\", 1),\n",
    "    (\"D04\", 12),\n",
    "    (\"B04\", 1),\n",
    "    (\"B03\", 10),\n",
    "    (\"B04\", 4),\n",
    "    (\"C02\", 4),\n",
    "    (\"D02\", 9),\n",
    "    (\"C04\", 9),\n",
    "    (\"D04\", 1),\n",
    "    (\"C02\", 6),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Inferred constants\n",
    "\n",
    "Constants inferred from the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGNIFICATION = \"60x\"\n",
    "file_name_struct = \"AssayPlate_Greiner_#655090_{0}_T0001F{1}L01A0{2}Z0{3}C0{2}.tif\"\n",
    "\n",
    "PATH_TO_OUTPUT = os.path.normpath(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer full path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bmidt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Multiple paths found! Using D:/hackathon\\60x images\\\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "_glob_struct = os.path.join(DATASET_PATH, MAGNIFICATION + \"*/\")\n",
    "_glob_results = glob.glob(_glob_struct)\n",
    "\n",
    "if len(_glob_results) == 0:\n",
    "    raise ValueError(\"No path found matching glob {0}\".format(_glob_struct))\n",
    "elif len(_glob_results) > 1:\n",
    "    from warnings import warn\n",
    "    warn(\"Multiple paths found! Using {0}\".format(_glob_results[-1]))\n",
    "\n",
    "PATH_TO_MAGNIFICATION = os.path.normpath(_glob_results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: \t D:\\hackathon\\60x images\n",
      "Saving results to: \t models\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading images from: \\t\", PATH_TO_MAGNIFICATION)\n",
    "print(\"Saving results to: \\t\", PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load train data\n",
    "\n",
    "We define a data pipeline for loading images from storage. This uses DeepTrack 2.0, and follows the structure of\n",
    "\n",
    "1. Load each z-slice of a well-site combination and concatenate them.\n",
    "2. Pad the volume such that the first two dimensions are multiples of 32 (required by the model).\n",
    "3. Correct for misalignment of the fluorescence channel and the brightfield channel (by a pre-calculated parametrization of the offset as a function of magnification and the site)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find all wells and sites\n",
    "\n",
    "We create an iterator over each well and site. `itertools.product` produces an iterator over each combination of its input. In this case, each site in each well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_and_sites = list(\n",
    "    itertools.product(\n",
    "        [\"B03\", \"B04\", \"C02\", \"C03\", \"C04\", \"D02\", \"D03\", \"D04\"],\n",
    "        range(1, 13) \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 81 images\n",
      "Validating on 15 images\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "random.shuffle(wells_and_sites)\n",
    "\n",
    "training_set = [w_s_tuple for w_s_tuple in wells_and_sites if w_s_tuple not in VALIDATION_WELLS_AND_SITES]\n",
    "validation_set = VALIDATION_WELLS_AND_SITES\n",
    "\n",
    "print(\"Training on {0} images\".format(len(training_set)))\n",
    "print(\"Validating on {0} images\".format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The root feature\n",
    "\n",
    "We use DeepTrack 2.0 to define the data loader pipeline. The pipeline is a sequence of `features`, which perform computations, controlled by `properties`, which are defined when creating the features. (Note that we any property with any name and value to a feature; if a property is not used by the feature, we refer to it as a dummy property.)\n",
    "\n",
    "The feature `root` is a `DummyFeature`, which is just a container of dummy properties and does not perform any computations.\n",
    "It takes the following arguments:\n",
    "\n",
    "* `well_site_tuple` is a dummy property that cycles through the well-site combinations in `wells_and_sites`\n",
    "* `well` is a dummy property that extracts the well from the `well_site_tuple`\n",
    "* `site` is a dummy property that extracts the site from the `well_site_tuple`\n",
    "\n",
    "Note that `well` and `site` are functions that take `well_site_tuple` as argument. These are dependent properties, and DeepTrack 2.0 will automatically ensure that they receive the correct input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterator = itertools.cycle(training_set)\n",
    "validation_iterator = itertools.cycle(validation_set)\n",
    "\n",
    "def get_next_well_and_site(validation):\n",
    "    if validation:\n",
    "        return next(validation_iterator)\n",
    "    else:\n",
    "        return next(training_iterator)\n",
    "\n",
    "# Accepts a tuple of form (well, site), and returns the well\n",
    "def get_well_from_tuple(well_site_tuple):\n",
    "    return well_site_tuple[0]\n",
    "\n",
    "# Accepts a tuple of form (well, site), and returns the site as \n",
    "# a string formated to be of length 3.\n",
    "def get_site_from_tuple(well_site_tuple):\n",
    "    site_string = \"00\" + str(well_site_tuple[1])\n",
    "    return site_string[-3:]\n",
    "\n",
    "\n",
    "\n",
    "root = dt.DummyFeature(\n",
    "    well_site_tuple=get_next_well_and_site,           # On each update, root will grab the next value from this iterator\n",
    "    well=get_well_from_tuple,                         # Grabs the well from the well_site_tuple\n",
    "    site=get_site_from_tuple,                         # Grabs and formats the site from the well_site_tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The brightfield image loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a brightfield stack. It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we take the properties of `root` (of importance `well` and `site`). The other properties of LoadImage will now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a shaped volume with dimensions (width, height, 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightfield_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_names=lambda well, site: [file_name_struct.format(well, site, 4, z) for z in range(1, 8)],\n",
    "    path=lambda file_names: [os.path.join(PATH_TO_MAGNIFICATION, file_name) for file_name in file_names]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 The fluorescence image loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a fluorescence stack. It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we take the properties of `root` (of importance `well` and `site`). The other properties of LoadImage will now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a shaped volume with dimensions (width, height, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluorescence_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_names=lambda well, site: [file_name_struct.format(well, site, action, 1) for action in range(1, 4)],\n",
    "    path=lambda file_names: [os.path.join(PATH_TO_MAGNIFICATION, file_name) for file_name in file_names],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Offset adjustment\n",
    "\n",
    "Offset adjustments using affine transformations. The offset is parametrized as a function of the site.\n",
    "\n",
    "`Combine` creates a feature that reslves and returns `brightfield_loader` and `fluorescence_loader`. We use this to calculate parameters of the datasets, including normalization coefficients and offset correction. \n",
    "\n",
    "Thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'005': [[-0.1912482656458341, -0.35492670110905444]],\n",
       " '002': [[-0.13342389404284588, -0.19368289013303325],\n",
       "  [-0.1285585708652387, -0.11550533162454946]],\n",
       " '012': [[-0.059404821391138424, -0.027027424057242]],\n",
       " '007': [[-0.30073960168997615, 0.002402578946546337],\n",
       "  [0.8501806144583512, -1.3233295723320313]],\n",
       " '008': [[0.031544170782479175, 0.11216320754410057]],\n",
       " '004': [[0.1906582941605317, 0.19709740154115513]],\n",
       " '009': [[-0.16578615417902107, 0.0072601176359276196]],\n",
       " '010': [[-0.404229033061417, 0.15959672102132716]]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feature = dt.Combine([brightfield_loader, fluorescence_loader])\n",
    "\n",
    "params = apdio.get_dataset_parameters(\"60x\", data_feature=data_feature, n_images=81)\n",
    "binned_offsets = params.bin(\"offset\", \"site\", reducer = np.mean)\n",
    "\n",
    "correct_offset.properties[\"translate\"] = dt.properties.Property()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties are set as follows:\n",
    "\n",
    "* `translate` sets how much we translate the image in pixels. It is a tuple representing the (x, y) shift. We calculate it as a function of the angular position of the site within the well, with site 1 at angle 0.\n",
    "* `root.properties` grabs the site from the root feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients of the regression\n",
    "\n",
    "correct_offset = dt.Affine(\n",
    "    translate=lambda site: binned_offsets[site],\n",
    "    **root.properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Define augmentations\n",
    "\n",
    "We use three kinds of augmentations: Mirroring (`deeptrack.FlipLR`), Affine transformations (`deeptrack.ElasticTransformation`), and Distortions (`deeptrack.Crop`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = dt.FlipLR()\n",
    "\n",
    "affine = dt.Affine(\n",
    "    rotate=lambda: np.random.rand() * 2 * np.pi,\n",
    ")\n",
    "\n",
    "corner = int(512 * (np.sqrt(2) - 1) / 2)\n",
    "cropping  = dt.Crop(\n",
    "    crop=(512, 512, None),\n",
    "    corner=(corner, corner, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create the pipeline \n",
    "\n",
    "We use the (`+`) operator to chain the features, defining the execution order. In DeepTrack 2.0, this means that the output of the feature on the left, is passed as the input to the feature on the right. This is done in the following steps:\n",
    "\n",
    "1. `corrected_brightfield` is generated by offsetting the `brightfield_loader`\n",
    "2. `data_pair` is created with input images and targets\n",
    "3. `augmented_data` are defined by using the augmentations as well as the cropping\n",
    "4. `validation_data` is created\n",
    "5. `dataset` is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_brightfield = brightfield_loader + correct_offset\n",
    "\n",
    "data_pair = dt.Combine([corrected_brightfield, fluorescence_loader])\n",
    "\n",
    "padded_crop_size = int(512 * np.sqrt(2))\n",
    "\n",
    "cropped_data = dt.Crop(\n",
    "    data_pair,\n",
    "    crop=(padded_crop_size, padded_crop_size, None),\n",
    "    updates_per_reload=16,\n",
    "    corner=lambda: (*np.random.randint(5000, size=2), 0),\n",
    ")\n",
    "\n",
    "augmented_data = cropped_data + affine + cropping\n",
    "\n",
    "validation_data = data_pair + dt.PadToMultiplesOf(multiple=(32, 32, None))\n",
    "\n",
    "dataset = dt.ConditionalSetFeature(\n",
    "    on_true=validation_data,\n",
    "    on_false=augmented_data,\n",
    "    condition=\"is_validation\",\n",
    "    is_validation=lambda validation: validation\n",
    ") + dt.AsType(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define generator\n",
    "\n",
    "We use generators to interface DeepTrack 2.0 features with Keras training routines. In DeepTrack 2.0, we have defined some special generators that speed up training. Here, we will use `deeptrack.ContinuousGenerator`, which continuosly geenrate augmented training images and makes them available for training the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = dt.generators.ContinuousGenerator(\n",
    "    dataset,\n",
    "    batch_function=lambda image: image[0],\n",
    "    label_function=lambda image: image[1],\n",
    "    batch_size=8,\n",
    "    min_data_size=100,\n",
    "    max_data_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define model\n",
    "\n",
    "Here, we use a GAN with a U-Net generator and a convolutional encoder discriminator. The generator is trained on MSE of the GAN error and MAE of the pixel difference. The discriminator is trained using MSE loss.\n",
    "\n",
    "More details are provided in the report.\n",
    "\n",
    "We also compile the assembled GAN with metrics, which include feature-wise MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generator() missing 1 required positional argument: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bba5784fc056>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGAN_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapido\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGENERATOR_BREADTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGENERATOR_DEPTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mGAN_discriminator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapido\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDISCRIMINATOR_DEPTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m GAN = dt.models.cgan(\n\u001b[0;32m      5\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGAN_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generator() missing 1 required positional argument: 'config'"
     ]
    }
   ],
   "source": [
    "GAN_generator = apido.generator(GENERATOR_BREADTH, GENERATOR_DEPTH)\n",
    "GAN_discriminator = apido.discriminator(DISCRIMINATOR_DEPTH)\n",
    "\n",
    "GAN = dt.models.cgan(\n",
    "    generator=GAN_generator,\n",
    "    discriminator=GAN_discriminator,\n",
    "    discriminator_loss=\"mse\",\n",
    "    discriminator_optimizer=Adam(lr=0.0002, beta_1=0.5),\n",
    "    assemble_loss=[\"mse\", \"mae\"],\n",
    "    assemble_optimizer=Adam(lr=0.0002, beta_1=0.5),\n",
    "    assemble_loss_weights=[\n",
    "        1 - MAE_LOSS_WEIGHT,\n",
    "        MAE_LOSS_WEIGHT\n",
    "    ],\n",
    ")\n",
    "\n",
    "GAN.compile(loss=\"mae\", metrics=apido.metrics(\"60x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model\n",
    "\n",
    "We execute the dataset pipeline, setting the option `validation` to be True in the update step. This toggles the ConditionalSetFeature to skip the augmentation, as well as making `root` draw the site-tuple from the validation set.\n",
    "\n",
    "Following this we initialize the generator using the `with` statements, which launches a thread to generate `min_data_size` samples before starting training. Finally `fit` starts the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs = []\n",
    "validation_targets = []\n",
    "\n",
    "for _ in range(len(validation_set)):\n",
    "    data_tuple = dataset.update(validation=True).resolve()\n",
    "    validation_inputs.append(data_tuple[0])\n",
    "    validation_targets.append(data_tuple[1])\n",
    "\n",
    "\n",
    "with generator:\n",
    "    h = GAN.fit(\n",
    "        generator, \n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(\n",
    "            np.array(validation_inputs),\n",
    "            np.array(validation_targets)\n",
    "        ),\n",
    "        validation_batch_size=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize validation set\n",
    "\n",
    "We plot and show model predictions for each image in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = GAN.generator.predict(np.array(validation_inputs), batch_size=4)\n",
    "\n",
    "for brightfield, targets, prediction in zip(validation_inputs, validation_targets, prediction):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(brightfield[:, :, 0])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(2,5,3)\n",
    "    plt.imshow(targets[:, :, 0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2,5,4)\n",
    "    plt.imshow(targets[:, :, 1])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2,5,5)\n",
    "    plt.imshow(targets[:, :, 2])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(2,5,8)\n",
    "    plt.imshow(prediction[:, :, 0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2,5,9)\n",
    "    plt.imshow(prediction[:, :, 1])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2,5,10)\n",
    "    plt.imshow(prediction[:, :, 2])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save model\n",
    "\n",
    "We save the generator and the discriminator seperately. The folder is named after the current date, the user which is running the notebook, and the current magnification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = apido.get_checkpoint_name(\"60x\")\n",
    "\n",
    "# Save generator\n",
    "generator_checkpoint_path = os.path.join(PATH_TO_OUTPUT, folder_name, \"generator_checkpoint\")\n",
    "os.makedirs(generator_checkpoint_path, exist_ok=True)\n",
    "GAN.generator.save(generator_checkpoint_path)\n",
    "\n",
    "# Save discriminator\n",
    "discriminator_checkpoint_path = os.path.join(PATH_TO_OUTPUT, folder_name, \"discriminator_checkpoint\")\n",
    "os.makedirs(discriminator_checkpoint_path, exist_ok=True)\n",
    "GAN.discriminator.save(discriminator_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
